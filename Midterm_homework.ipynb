{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Midterm_homework.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshay594/fastai-projects/blob/master/Midterm_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uRXEqrUmax0",
        "colab_type": "text"
      },
      "source": [
        "# TogetherML `fastai` midterm - First Round - October 2019\n",
        "\n",
        "This midterm consists of some multiple choice questions, some full answer questions and a final coding question that uses what you know of `fastai` and PyTorch from the course to build a very simple sentiment classifier using Logistic Regression.\n",
        "\n",
        "* For the full answer questions, there will be cells for you to edit so you can place your answer accordingly.\n",
        "* For the multiple choice questions, for the cell where you answer the question, simply write in the number associated with the answer you are choosing.  No justification of why you chose that answer is necessary.\n",
        "* For the final coding question, simply edit the corresponding functions with your answer, then run the training cells and `unittest` cells until your cells finally pass the tests.  This will ensure that your answers are correct.\n",
        "\n",
        "The due date for this is November 4th, 2019 at 11:59 p.m. PST (GMT-8).  \n",
        "\n",
        "To submit this midterm, please send me (`@rayryeng`) an individual message on Discord with your answers to this midterm as a separate notebook.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSOSEHXWm_6l",
        "colab_type": "text"
      },
      "source": [
        "## Question #1\n",
        "\n",
        "Describe in your own words what \"transfer learning\" means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZHgnKtMnvy7",
        "colab_type": "text"
      },
      "source": [
        "Answer: Transfer learning essentially is using pretrained models for training the new data and make predictions on the new data. In layman terms, it's like a father is passing learnings of his entire life to his offspring."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fX_jIYhn3Qo",
        "colab_type": "text"
      },
      "source": [
        "## Question #2\n",
        "\n",
        "Describe in your own words what it means for a model to \"overfit\" and provide two suggestions on how to combat overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJTCGE9VoHHs",
        "colab_type": "text"
      },
      "source": [
        "Overfitting occurs when a model's training loss is way lesser than it's validation loss. In layman terms, Suppose you have ripped by a taxi driver in Bombay and now you are saying that every taxi driver in Bombay is a fraud based upon your one negative encounter.\n",
        "\n",
        "\n",
        "Two Suggestions for dealing with Overfitting:\n",
        "\n",
        "1. Regularization: It helps us in penalizing the higher weights in the feature matrix, which eventually leads to less biased data. Since overfitting occurs when the variance in the weights are very high. \n",
        "\n",
        "2. Training on holdout data: Suppose you have split the data into `k` folds, now what you can use `k-1` for training the model and `1` fold as a holdout data. This allows you to keep your holdout set as a truly unseen dataset for selecting your final model.\n",
        "\n",
        "#### honorable mentions:\n",
        "\n",
        "*   Using `Dropout` when using Neural Network\n",
        "*   Removing Features\n",
        "*   Collect more data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1ODoIMPoT79",
        "colab_type": "text"
      },
      "source": [
        "## Question #3\n",
        "\n",
        "What does it mean when a network is frozen in `fastai`.  What about unfrozen?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jKHQ0jF5pBmL"
      },
      "source": [
        "**Freezing**: When we don't want to train particular layers of a neural network. Because when we are `freezing` the layers you are essentially saying don't do backpropagation in those frozen layers, this process freezes only randomly generated layers.\n",
        "\n",
        "\n",
        "**Unfreezing**: When our model is ready after freezing phase then we can chain the entire network and train it with the help of concept called *Discriminative Learning Rates.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xJaVSRUpC3l",
        "colab_type": "text"
      },
      "source": [
        "## Question #4\n",
        "\n",
        "When using `lr_find` on your model, how do you decide what the best learning rate is when frozen?  What about when the network is unfrozen?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UKJeojNKpnmw"
      },
      "source": [
        "Finding the learning rate is a crucial step since this a `speed` for the model to train on the fed data. We should go for a learning rate when training loss is lowest or steepest if you are seeing learning rate on a graph.\n",
        "\n",
        "When it comes to choosing the learning rate for an unfrozen layer we have to consider the stage where it was frozen also because those layers won't be needing much training then the newly added layers. That is why we use *Discriminative Learning Rates*. It essentially means, **training the network on two learning rates**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL8bNxJWppHn",
        "colab_type": "text"
      },
      "source": [
        "## Question #5\n",
        "\n",
        "When putting your model to production, why is it not a good idea to use GPUs when predicting the class of your input image?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fGesxdFPqO1x"
      },
      "source": [
        "It will be hard to scale compared to a  CPU inference. It does not make any sense either if you don't have good traffic on your website and CPU inference will be much cheaper compared to GPU inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DGlEu9nSqPYD"
      },
      "source": [
        "## Question #6\n",
        "\n",
        "Explains what happens in your model training when the learning rate is too high.  What about when it's too low?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ltcvVg6DqdyB"
      },
      "source": [
        "When the learning rate is too high your model is highly vulnerable for skipping the global optimum value.\n",
        "\n",
        "When it's too low it may even fail to converge for global optimum value, it will be like a snail is trying to climb down from a mountain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B_m31XPEqeES"
      },
      "source": [
        "## Question #7\n",
        "\n",
        "What is the difference between semantic segmentation and instance segmentation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GXeafFQ3rKcC"
      },
      "source": [
        "**Semantic Segmentation** : It is a technique which detetcts each pixel from the object category given that all labels are available for the model to train on.\n",
        "\n",
        "**Instance Segmentation** : same as Semantic Segmentation, but dives a bit deeper, it identifies , for each pixel, the object instance it belongs to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R_MRDxeErLZN"
      },
      "source": [
        "## Question #8\n",
        "\n",
        "Running training for a very long time or a very large number of epochs is usually a good thing for a deep neural network\n",
        "\n",
        "1. True\n",
        "2. False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDjeCerRJCab",
        "colab_type": "text"
      },
      "source": [
        "False\n",
        "\n",
        "Reason: If we have *enough data* then we may train our network for longer period of time otherwise it will lead to overfitting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpXN7EMGJjqx",
        "colab_type": "text"
      },
      "source": [
        "## Question #9\n",
        "When is it a good idea to train in mixed-precision mode (i.e. 16-bit instead of 32-bit)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "siSGNdQMJtwY"
      },
      "source": [
        "When running out of memory a lot we can use Mixed Preicision. It's doing calculations on 16 bit precision instead of 32 bit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmkUAe6KKwPI",
        "colab_type": "text"
      },
      "source": [
        "## Question #10\n",
        "\n",
        "Why do we prefer using ReLU activation functions for deep networks over other conventional ones, such as sigmoid or tanh?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pXOBiRjqK4X_"
      },
      "source": [
        "`ReLU` squashes down the negative weights whihch means if a function outputs a positive value it's going to give us directly else it will output a zero. It makes it quick to evaluate and if the neuron (input) is not dead, then updates are fairly easy on the weight matrix. That is why it never saturates since it's gradient is always 1.\n",
        "\n",
        "Compare this to sigmoid or tanh, both of which will saturate, if the weights are in higer variance or in low then gradient will change quickly which sometimes can become quick or slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t69OX_qSLyOT",
        "colab_type": "text"
      },
      "source": [
        "## Question #11\n",
        "\n",
        "In your own words, explain what a Dropout layer is and what specifically does it mean when the dropout rate is 0.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sFsAYBZaL8Vj"
      },
      "source": [
        "We remove `randomly` some activations from the network, it's like regularization but instead of penalizing the weights we penalize `activations` with some percentage value.\n",
        "\n",
        "When we are saying dropout rate is 0.2 then we are saying we need to ignore or drop next 20% of activation layer in the network.\n",
        "\n",
        "Let's say we have 5 nodes in our neural network then by putting `dropout=0.2` we are asking for one in 5 inputs will be randomly excluded from each update cycle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3vtFWQnBL-Jv"
      },
      "source": [
        "## Question #12\n",
        "\n",
        "In your own words, explain what weight decay does to a model. What if it's too high? Too low?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDV11bmzOkoH",
        "colab_type": "text"
      },
      "source": [
        "When the loss function add to it the sum of the squares of parameters multiplied by some number `wd`, where `wd` is **Weight Decay**. If the value of `wd` is too high then it will penalize the parameters too much which will lead to **Underfitting** and if it is too low then it will lead to **Overfitting**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c94LTn_VK46V"
      },
      "source": [
        "## Question #13\n",
        "\n",
        "As we go deeper in a neural network, which of the following choices is *false*?\n",
        "\n",
        "1. The deeper the network is, the harder it is to train.\n",
        "2. If we make a neural network deep enough, we can satisfy the universal approximation theorem where any function can be expressed by a neural network\n",
        "3. Adam optimization is always better than Momentum\n",
        "4. The deeper you go, the more high level or abstract the filters get in picking up features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qNuyxMJcOrHi"
      },
      "source": [
        "2. (It needs `1` hidden layer to approximate any function in a neural network)\n",
        "3. Curves of SGDs on valid data are smoother after n epochs but on adaptive optimizers we have larger update on parameters due to it's adaptive nature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_wD48leOsGf",
        "colab_type": "text"
      },
      "source": [
        "## Question #14\n",
        "\n",
        "Batch Normalization addresses the issue of covariate shift between layers in a deep neural network\n",
        "\n",
        "1. True\n",
        "2. False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vOCR1OEbOz_D"
      },
      "source": [
        "True\n",
        "\n",
        "Reason: The problem appears in the intermediate layers because the distribution of the activations is constantly changing during training. This slows down the training process because each layer must learn to adapt themselves to a new distribution in every training step. **This problem is known as internal covariate shift.**\n",
        "\n",
        "Reference: https://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sldK0sDGPKgM",
        "colab_type": "text"
      },
      "source": [
        "## Question #15\n",
        "\n",
        "Why is the embedding matrix so important in collaborative filtering?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p_MMDoWsPbCL"
      },
      "source": [
        "Embedding matrix is just like another matrix but what makes them so special is that they help to find value(prediction) based upon bias and parameters. \n",
        "\n",
        "Let's say you have an embedding matrices emb_1 and em_2 and two bias matrices b1 and b2. So the predicted value from these features will be.\n",
        "\n",
        "$$pred = dot(emb_1, emb_2) + b1 + b2$$\n",
        "\n",
        "When we signup for Netflix and stream some web series we start to get a recommendation based upon previously watched series that is where embedding matrix comes in play with collaborative filtering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wRm7EY08Pbhw"
      },
      "source": [
        "# Question #16\n",
        "\n",
        "Why is the bias parameter so important in classification models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TcKXZVmZPhya"
      },
      "source": [
        "In simple words suppose you have gone for a movie and you did not like it but the movie's rating on IMDB is more than 8, so now what bias will do for you deep neural network is it will ignore the IMDB rating and you will not see that movie in your recommended list anymore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AUj74maXPp-E"
      },
      "source": [
        "## Question #17\n",
        "\n",
        "Explain in your own words how the \"heat map\" works for convolutional neural networks and why it's useful.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IO2LudB7QAZw"
      },
      "source": [
        "Let's say you have a matrix called \n",
        "\n",
        "$$M = \\begin{bmatrix} \n",
        "a & b \\\\\n",
        "c & d \n",
        "\\end{bmatrix}$$ \n",
        "\n",
        "and a pixel matrix called\n",
        "\n",
        "$$P = \\begin{bmatrix} \n",
        "e & f \\\\\n",
        "g & h \n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Convolution matrix will be\n",
        "\n",
        "$$CM = \\begin{bmatrix} \n",
        "a*e & b*f \\\\\n",
        "c*g & d*h \n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Lastly we will add the each instance from the $CM$ matrix like this $V=a*e+b*f+c*g+d*h$\n",
        "\n",
        "The more the $V$ is positive the more it will shine or will tend to reach 255.0 value(for white color).\n",
        "\n",
        "It's useful to show the most activated paramters in pixel matrices.\n",
        "\n",
        "Suppose we have NXN grids across M channels and we wants to find out what is the value in those NXN grids, so instead of averaging across the NXN, we will average across M channels by doing this we will be having we will be having NXN single matrix and each grid of this matrix will show how much the area was activated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k7TqKVPOQHvj"
      },
      "source": [
        "# Question #18\n",
        "\n",
        "Suggest two ways to reduce the dimensionality of your data in order to better interpret what's going on overall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oaF4YrPEQ_up"
      },
      "source": [
        "1. PCA - Projecting the higher dimensional data onto a lower space.\n",
        "2. Factor Analysis - Reduces a large number of variables into a fewer number of factors.\n",
        "\n",
        "P.S. I am assuming that the questionnaire was asking for Linear Dimensionality Reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DET32e-sQ39Z",
        "colab_type": "text"
      },
      "source": [
        "## Question #19\n",
        "\n",
        "Suggest two ways to reduce the influence of outliers in your data when training your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c5KFp2zBRAHh"
      },
      "source": [
        "1. **Delete the value**: If you have only a few outliers, you may simply delete those values, so they become blank or missing values.\n",
        "\n",
        "2. **Delete the variable**: If transforming the value or variable does not eliminate the problem, you may want to simply delete the variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3jyAdfO2RA52"
      },
      "source": [
        "## Question #20\n",
        "\n",
        "Suppose we have an image as shown below:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "98 & 66 & 78 & 10 & 5 \\\\\n",
        "1 & 56 & 7 & 56 & 4 \\\\\n",
        "-8 & 6 & 76 & 3 & 51 \\\\\n",
        "10 & -16 & 3 & 49 & 13 \\\\\n",
        "4 & -6 & 23 & 17 & 38 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The coordinate $(x,y)$ references the horizontal and vertical coordinate in this image, starting with the top left corner being $(0,0)$ and the bottom right corner being $(4,4)$.  Using the 3 x 3 kernel shown below:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & -1\\\\\n",
        "2 & 0 & -2\\\\\n",
        "1 & 0 & -1\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Perform a convolution calculation with the kernel being centered at $(2,3)$.  Please write the final answer in the cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "trFrpWlATYOK"
      },
      "source": [
        "$$K(2, 3) = \\begin{bmatrix} \n",
        "6 & 76 & 3 \\\\\n",
        "-16 & 3 & 49 \\\\\n",
        "-6 & 23 & 17 \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$CM = \\begin{bmatrix} \n",
        "1 & 0 & -1 \\\\\n",
        "2 & 0 & -2 \\\\\n",
        "1 & 0 & -1 \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$V = K(2, 3) X CM(element wise)$$\n",
        "\n",
        "$$V = -150$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8JpHpA9TbJF",
        "colab_type": "text"
      },
      "source": [
        "# Coding Question - Bag of Words Features in Text for Sentiment Classification using Linear Classifiers\n",
        "\n",
        "In this coding question, we will explore how to classify whether a sentence is positive or negative using a bag of words approach.  In this question, the input is a list of lowercased strings with no punctuation for simplicity.  This list of strings will denote our *corpus*, or the total possible words in our universe of language for this question.\n",
        "\n",
        "Our objective is given this list of sentences as well as the ground truth labels for each sentence - positive being a \"good\" or \"happy\" sentence and negative being a \"bad\" or \"angry\" sentence, we want to be able to build a classification model that can successfully do this.  We will start with Logistic Regression, but with a twist where we'll use PyTorch to do the optimal weight calculations for us.\n",
        "\n",
        "We will complete this question in three stages:\n",
        "\n",
        "1. `word_to_index`: Create a dictionary where we map a word to a unique index\n",
        "2. `build_feature_matrix`: Build our feature matrix which converts the list of sentences into a feature matrix that would be used as input into Logistic Regression\n",
        "3. `find_params`: Use PyTorch and this input matrix from (2) to find the optimal weights using Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBzbgLSKV6vj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell so we can import all of the packages we need for this\n",
        "# question.  You can use the following packages to complete this coding\n",
        "# question and no others\n",
        "\n",
        "import numpy as np\n",
        "import unittest\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30kDLudXV1rt",
        "colab_type": "text"
      },
      "source": [
        "## `word_to_index`\n",
        "\n",
        "Please carefully read the signature in the function below and complete it.  You can then run the `unittest` after to make sure you got the function right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5GbHLxjJAng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_to_index(sentences):\n",
        "    \"\"\"\n",
        "    Function that computes a dictionary where the key is a unique word\n",
        "    and the value is a unique ID or index for the word.  This function\n",
        "    takes in a list of sentences and processes each word in the sentence\n",
        "    to compute this dictionary\n",
        "\n",
        "    For example, supposing if our list was:\n",
        "    [\n",
        "        \"this movie was awesome\",\n",
        "        \"i didn't like this movie\"\n",
        "    ]\n",
        "\n",
        "    The dictionary would contain:\n",
        "    {\n",
        "        \"this\": 0,\n",
        "        \"movie\": 1,\n",
        "        \"was\": 2,\n",
        "        \"awesome\": 3\n",
        "        \"i\": 4\n",
        "        \"didn't\": 5\n",
        "        \"like\": 6\n",
        "    }\n",
        "\n",
        "    Note that the order of the index is based on when we encounter the word\n",
        "    in any sentence.\n",
        "\n",
        "    Input -\n",
        "        A list of sentences as str that are space delimited.  Assume that\n",
        "        each word between spaces is complete, including punctuation.\n",
        "\n",
        "    Output -\n",
        "        A dictionary according to the specifications laid out above\n",
        "    \"\"\"\n",
        "    unique_words = []\n",
        "    for words in sentences:\n",
        "      temp = words.split(\" \")\n",
        "      for w in temp:\n",
        "        if w not in unique_words:\n",
        "          unique_words.append(w)\n",
        "    \n",
        "    # Initialization of numbers for words contains in bag\n",
        "    bag = dict()\n",
        "    for i in range(len(unique_words)):\n",
        "      bag[unique_words[i]] = i\n",
        "    \n",
        "    return bag\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SugWykOQWcJO",
        "colab_type": "code",
        "outputId": "2f65be4c-59b3-4ee5-8cce-e40d923ed98f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "class TestSentiment(unittest.TestCase):\n",
        "    def test_word_to_index(self):\n",
        "        sentences = [\n",
        "            \"this movie was awesome\",\n",
        "            \"i didn't like this movie\"\n",
        "        ]\n",
        "\n",
        "        out = word_to_index(sentences)\n",
        "        self.assertTrue(len(out) == 7)\n",
        "        self.assertEqual(out[\"this\"], 0)\n",
        "        self.assertEqual(out[\"movie\"], 1)\n",
        "        self.assertEqual(out[\"was\"], 2)\n",
        "        self.assertEqual(out[\"awesome\"], 3)\n",
        "        self.assertEqual(out[\"i\"], 4)\n",
        "        self.assertEqual(out[\"didn't\"], 5)\n",
        "        self.assertEqual(out[\"like\"], 6)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.001s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6S7-oYnXSDw",
        "colab_type": "text"
      },
      "source": [
        "## `build_feature_matrix`\n",
        "\n",
        "Please carefully read the signature in the function below and complete it. You can then run the unittest after to make sure you got the function right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg6vAoyNW697",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter \n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "\n",
        "def build_feature_matrix(sentences):\n",
        "    \"\"\"\n",
        "    Function that computes a feature matrix of size M x N where M is the\n",
        "    number of sentences and N is the number of unique words in our corpus.\n",
        "    In particular, each row is the corresponding feature representation of\n",
        "    the sentence and each element (i, j) of this matrix calculates the\n",
        "    NORMALIZED frequency of occurrence of the word j in sentence i by\n",
        "    dividing each row by its L2 norm.\n",
        "\n",
        "    For example, if our sentence corpus only consisted of:\n",
        "\n",
        "    [\n",
        "        \"hello hello there\",\n",
        "        \"i'm i'm fine fine fine\"\n",
        "    ]\n",
        "\n",
        "    The matrix would look like:\n",
        "\n",
        "    hello there i'm fine\n",
        "    [  2    1    0   0]\n",
        "    [  0    0    2   3]\n",
        "\n",
        "    The L2 norm of the first and second row are sqrt(5) and sqrt(13)\n",
        "    respectively, and so the expected feature matrix would be:\n",
        "\n",
        "        hello         there     i'm         fine\n",
        "    [  2/sqrt(5)    1/sqrt(5)    0            0      ]\n",
        "    [     0             0     2/sqrt(13)   3/sqrt(13)]\n",
        "\n",
        "    Each column is referenced by the index described by the word from the\n",
        "    dictionary produced by word_to_index\n",
        "\n",
        "    Input -\n",
        "        A list of sentences as str that are space delimited.  Assume that\n",
        "        each word between spaces is complete.\n",
        "    Output -\n",
        "        A tuple that contains the feature matrix as a floating-point numpy\n",
        "        array and the corresponding dictionary mapping the word to an index.\n",
        "    \"\"\"\n",
        "    wtoi = word_to_index(sentences)\n",
        "\n",
        "    matrix = []\n",
        "    for w in sentences:\n",
        "      # Words that exist in dict according to their occurence\n",
        "      temp_dict = Counter(w.split(\" \"))\n",
        "      temp = []\n",
        "      for w_ in wtoi:\n",
        "        temp.append(temp_dict[w_])\n",
        "      matrix.append(temp)\n",
        "    \n",
        "    ## L2 normalization\n",
        "    for i in range(len(matrix)):\n",
        "      total = sum(i**2 for i in matrix[i])\n",
        "      for j in range(len(matrix[i])):\n",
        "        matrix[i][j] = matrix[i][j]/np.sqrt(total)\n",
        "\n",
        "    matrix = [np.array(matrix), wtoi]\n",
        "    return tuple(matrix)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aYS75V8YDi0",
        "colab_type": "code",
        "outputId": "6f63af2f-b57f-4fb9-e0e2-5b6040178be4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "class TestSentiment(unittest.TestCase):\n",
        "    def test_build_features(self):\n",
        "        sentences = [\n",
        "            \"this movie was awesome\",\n",
        "            \"i didn't like this movie\"\n",
        "        ]\n",
        "\n",
        "        out = build_feature_matrix(sentences)\n",
        "        self.assertTrue(type(out) == tuple)\n",
        "        self.assertTrue(len(out) == 2)\n",
        "        features = out[0]\n",
        "        self.assertTrue(type(features) == np.ndarray)\n",
        "        self.assertTrue(features.dtype == np.float)\n",
        "        self.assertTrue(features.shape == (2, 7))\n",
        "        compare_to = np.zeros((2, 7))\n",
        "        compare_to[0,:4] = 0.5\n",
        "        compare_to[1,4:] = 1.0 / np.sqrt(5)\n",
        "        compare_to[1,:2] = 1.0 / np.sqrt(5)\n",
        "        np.testing.assert_allclose(features, compare_to)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.004s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k25jQjycYoAF",
        "colab_type": "text"
      },
      "source": [
        "# `find_params`\n",
        "\n",
        "`find_params` will ultimately train your model using Stochastic Gradient Descent.  Before we do this, we need to define some moving pieces before ultimately doing this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9cFZYI_4u3J",
        "colab_type": "text"
      },
      "source": [
        "### Step #1\n",
        "First define a class called `Logistic` that inherits from the `torch.nn.Module` class where the constructor consists of a standard `torch.nn.Linear` layer combined with a `torch.nn.Sigmoid` layer wrapped around it.  Logistic Regression can be thought of as a single layer network with no hidden layers.  You will need to design the constructor so that it creates a linear layer with the total number of neurons being the total number of number of words in the dictionary.  You'll need to provide an input into the constructor that specifies the total number of neurons.  It will also be a good idea to add a bias unit to the linear layer for a high degree of accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDOVM5le5V9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Logistic(torch.nn.Module):\n",
        "    def __init__(self, n):\n",
        "        # n is the total number of input neurons for the Linear layer\n",
        "        super().__init__()\n",
        "        self.lin = torch.nn.Linear(n, 1, bias=True)\n",
        "        self.out = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, xb):\n",
        "        x = self.lin(xb)\n",
        "        y = self.out(x)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmbGzKaU5lYw",
        "colab_type": "text"
      },
      "source": [
        "### Step #2\n",
        "Define the appropriate loss function and store it in a variable called `loss_func` that is expected of logistic regression.  Take note that this is a *binary* classifier, and because this is classification, the cross entropy loss function is the ideal one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeEdm-pIatfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func = torch.nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQG14BC358EY",
        "colab_type": "text"
      },
      "source": [
        "### Step #3\n",
        "\n",
        "Now finally complete the behaviour for `find_params` shown below and read the docstring carefully.  Make sure you use the `loss_func` and `Logistic` class you just made to do this work.\n",
        "\n",
        "*Hints*:\n",
        "\n",
        "* Be careful when handling NumPy arrays as they require being converted to Torch tensors before proceeding\n",
        "* Because the weights are randomly initialized each time, don't pay attention to the actual magnitude of the weights.  Focus on classifying the actual sentences correctly.\n",
        "* Make sure you use the variable and class you created in Steps #1 and #2 here in your final solution.\n",
        "* Have a look at Lesson #5 to get a good overview of how to write this function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mvWmXF0YIQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_params(sentences, y, num_iter=1000, learning_rate=0.1):\n",
        "    \"\"\"\n",
        "    Finds the optimal weights of the inference model for classifying\n",
        "    between positive and negative sentences using Logistic Regression.\n",
        "\n",
        "    Inputs -\n",
        "        sentences - A list sentences as str that are space delimited.\n",
        "        Assume that each word between spaces is complete with no punctuation.\n",
        "        This list is N elements long corresponding to the number of sentences.\n",
        "        y - A M x 1 numpy array of labels where 0 is a negative sentiment\n",
        "        and 1 is a positive sentiment - each index into this array corresponds\n",
        "        to the sentence it describes.\n",
        "        num_iter - The number of iterations for logistic regression to be\n",
        "        learned by gradient descent.\n",
        "        learning_rate - The learning rate for gradient descent\n",
        "    Outputs -\n",
        "        A tuple that contains the trained PyTorch model and a NumPy\n",
        "        array that records the losses of your model per iteration\n",
        "    \"\"\"\n",
        "    n = len(word_to_index(sentences).keys())\n",
        "    y = torch.FloatTensor(y).reshape(-1, 1)\n",
        "    model = Logistic(n)\n",
        "    features = torch.FloatTensor(build_feature_matrix(sentences)[0])\n",
        "    losses = []\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    model.train()\n",
        "    \n",
        "    for i in range(num_iter):\n",
        "      prediction = model(features)\n",
        "      # computing the difference\n",
        "      loss = loss_func(prediction, y)\n",
        "      # Computing gradients\n",
        "      loss.backward()\n",
        "      #updating the weights\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      # storing the loss values\n",
        "      losses.append(loss.item())\n",
        "\n",
        "    return tuple([model, losses])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omwBB9WX7TNv",
        "colab_type": "text"
      },
      "source": [
        "## Step #4\n",
        "\n",
        "Defined below are some sentences as well as the corresponding positive or negative sentiment that accompanies each sentence.\n",
        "\n",
        "Play around with the learning rate and total number of iterations until you feel confident that your model has learned the system properly.  We won't concern ourselves with training and validation split.  Just get your model to classify all of the sentences correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzQ-6O4EaGN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\"jaris is a fraud\",\n",
        "             \"i love that jaris is being exposed\",\n",
        "             \"jaris is a piece of crap\",\n",
        "             \"siraj plagiarizes and gets away with it\",\n",
        "             \"he needs to stay off twitter\",\n",
        "             \"i am so happy that siraj is getting the karma he deserves\",\n",
        "             \"i love ai\",\n",
        "             \"i love pytorch\"\n",
        "             \"jeremy howard is an amazing man\",\n",
        "             \"rachel thomas is my idol\",\n",
        "             \"siraj is a script kiddie\"]\n",
        "labels = [[0, 1, 0, 0, 0, 1, 1, 1, 1, 0]]\n",
        "labels = np.array(labels).astype(np.bool).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zjP2ReAaoWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Play with the learning here\n",
        "model, losses = find_params(sentences, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxhJf7M48c7l",
        "colab_type": "text"
      },
      "source": [
        "Here's some code that plots the losses over the iterations.  Run this cell as you're iterating with the above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ8NK8zZ8bjV",
        "colab_type": "code",
        "outputId": "34a0b036-2263-4dbc-f055-d30a65313936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.arange(len(losses)), losses)\n",
        "plt.grid()\n",
        "plt.xlabel('Iteration #')\n",
        "plt.ylabel('Loss function')\n",
        "_ = plt.title('Loss function vs. # of iterations')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN7vJVc880mb",
        "colab_type": "text"
      },
      "source": [
        "Using the returned model above, rerun the predictions on the input sentences and display how they were classified.  Make sure you build the feature matrix and convert to a Torch tensor before doing so.  Compare these to your ground truth labels.  Did your model successfully classify the sentences?\n",
        "\n",
        "Try putting in different sentences that were not part of the training and see how it performs.  What can you do to improve performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85F8Plxa5vXr",
        "colab_type": "code",
        "outputId": "30c72a30-add2-4e93-a6de-78dd1e62772a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "sentences = [\"I like computer vision\",\n",
        "             \"I'm iron man.\",\n",
        "             \"Siraj is a scum.\",\n",
        "             \"I am a terrible questionnaire.\"]\n",
        "\n",
        "labels = [[1, 1, 0, 0]]\n",
        "labels = np.array(labels).astype(np.bool).T\n",
        "\n",
        "model, losses = find_params(sentences, labels)\n",
        "\n",
        "mat = torch.FloatTensor(build_feature_matrix(sentences)[0])\n",
        "model(mat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7923],\n",
              "        [0.8281],\n",
              "        [0.0825],\n",
              "        [0.1001]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeAIxnze-sCN",
        "colab_type": "text"
      },
      "source": [
        "## Improving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC0pGlhy6P1W",
        "colab_type": "code",
        "outputId": "08f1f8fa-3acc-49ab-96b9-03b24223ec73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "def find_params_with_adam(sentences, y, num_iter=1000, learning_rate=0.1):\n",
        "    \"\"\"\n",
        "    Finds the optimal weights of the inference model for classifying\n",
        "    between positive and negative sentences using Logistic Regression.\n",
        "\n",
        "    Inputs -\n",
        "        sentences - A list sentences as str that are space delimited.\n",
        "        Assume that each word between spaces is complete with no punctuation.\n",
        "        This list is N elements long corresponding to the number of sentences.\n",
        "        y - A M x 1 numpy array of labels where 0 is a negative sentiment\n",
        "        and 1 is a positive sentiment - each index into this array corresponds\n",
        "        to the sentence it describes.\n",
        "        num_iter - The number of iterations for logistic regression to be\n",
        "        learned by gradient descent.\n",
        "        learning_rate - The learning rate for gradient descent\n",
        "    Outputs -\n",
        "        A tuple that contains the trained PyTorch model and a NumPy\n",
        "        array that records the losses of your model per iteration\n",
        "    \"\"\"\n",
        "    n = len(word_to_index(sentences).keys())\n",
        "    y = torch.FloatTensor(y).reshape(-1, 1)\n",
        "    model = Logistic(n)\n",
        "    features = torch.FloatTensor(build_feature_matrix(sentences)[0])\n",
        "    losses = []\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    model.train()\n",
        "    \n",
        "    for i in range(num_iter):\n",
        "      prediction = model(features)\n",
        "      # computing the difference\n",
        "      loss = loss_func(prediction, y)\n",
        "      # Computing gradients\n",
        "      loss.backward()\n",
        "      #updating the weights\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      # storing the loss values\n",
        "      losses.append(loss.item())\n",
        "\n",
        "    return tuple([model, losses])\n",
        "\n",
        "sentences = [\"jaris is a fraud\",\n",
        "             \"i love that jaris is being exposed\",\n",
        "             \"jaris is a piece of crap\",\n",
        "             \"siraj plagiarizes and gets away with it\",\n",
        "             \"he needs to stay off twitter\",\n",
        "             \"i am so happy that siraj is getting the karma he deserves\",\n",
        "             \"i love ai\",\n",
        "             \"i love pytorch\"\n",
        "             \"jeremy howard is an amazing man\",\n",
        "             \"rachel thomas is my idol\",\n",
        "             \"siraj is a script kiddie\"]\n",
        "labels = [[0, 1, 0, 0, 0, 1, 1, 1, 1, 0]]\n",
        "labels = np.array(labels).astype(np.bool).T\n",
        "\n",
        "model, losses = find_params_with_adam(sentences, labels)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.arange(len(losses)), losses)\n",
        "plt.grid()\n",
        "plt.xlabel('Iteration #')\n",
        "plt.ylabel('Loss function')\n",
        "_ = plt.title('Loss function vs. # of iterations')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14cWoY79tyiQ",
        "colab_type": "text"
      },
      "source": [
        "We can also use \n",
        "\n",
        "\n",
        "\n",
        "1.   Embeddings\n",
        "2.   Dropout\n",
        "3.   Adding more data\n",
        "\n",
        "for improving the quality of our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8CDwZPvwFd1",
        "colab_type": "text"
      },
      "source": [
        "## The End"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JNfaWalwJmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
